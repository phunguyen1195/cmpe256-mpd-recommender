# -*- coding: utf-8 -*-
"""Spotify_MF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QpZG-C4wjomvf9ugEffMM4-DbkiBH6kD
"""

import numpy as np 
import pandas as pd 
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from scipy import sparse
import random
!pip install lightfm
import lightfm 
from lightfm import LightFM, cross_validation
from lightfm.evaluation import precision_at_k, auc_score
from sklearn.metrics.pairwise import cosine_similarity
from google.colab import drive
drive.mount("/content/drive")

# import pandas as pd
# df=pd.read_csv('/content/drive/MyDrive/CMPE256/data_csv/mpd.slice.0-999.csv')
# df.head(5)
import glob
import os

path = r'/content/drive/MyDrive/CMPE256/data_csv/10slides' # use your path
all_files = glob.glob(os.path.join(path, "*.csv"))

df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)

df

df.isna().sum()

df['track_uri'].isna().sum()

df.drop_duplicates(subset = 'track_uri')['track_name'].value_counts()

df.to_csv('10kclean.csv', encoding='utf-8', index=False)

#Creating matrix 
n_pls = df.playlist_pid.unique().shape[0]
n_songs = df.track_uri.unique().shape[0]

print(f'Num playlists: {n_pls}')
print(f'Num unique songs: {n_songs}')
print(f'Sparsity: {100 - float(df.shape[0]) /float(n_pls*n_songs) * 100:0.3f}%')

cols = df.playlist_pid.astype('category').cat.codes 
rows = df.track_uri.astype('category').cat.codes

cols.unique().shape

likes = np.ones(df.shape[0]) #1s if song is in playlist
likes = sparse.csr_matrix((likes, (rows, cols)), dtype=np.float64) #make sparse matrix

print(df.groupby(by=['playlist_pid', 'track_uri']).count()['track_name'].shape)

print(f'min num songs in playlist: {likes.sum(axis=1).min()}')
print(f'min num playlists song is featured in: {likes.sum(axis=0).min()}')

likes = likes.tocsc()

def train_test_split(likes, num_to_mask, rd_seed=random.seed(0)):
  '''
  Make training set with masked interactions

  likes: the full sparse matrix of playlist-song interactions
  num_to_mask: how many interactions we will mask for our training set
  rd_seed: random.seed to use, default 0

  Inspired by: https://www.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/
  '''
  #make the train and test matrices
  #use lil matrix for efficiency
  train = likes.copy().tolil() #copy all the data to train for later masking
  test = likes.copy().tolil()
  #test = sparse.lil_matrix(likes.shape, dtype=np.float64) #initialize an empty matrix

  for pl in np.arange(0, likes.shape[1]): #for each playlist
    if len(likes.getcol(pl).indices) >= 20: #playlist must have 20+ unique songs
      #mask a random subset of 10 'likes' (1s)
      masked_likes = np.random.choice(likes.getcol(pl).indices, 
                                        size=num_to_mask, 
                                        replace=False)
      train[masked_likes, pl] = 0.

      #mask everything but these likes in the test set
      #scipy has shape assignment mismatch when trying to assign column values
      #from likes matrix to test matrix, even though shapes are equivalent
      cells_to_mask = likes.getcol(pl).indices[~np.isin(likes.getcol(pl).indices, masked_likes)]
      test[cells_to_mask, pl] = 0.

    else: #mask everything from the test set if playlist is under 20 unique songs
      cells_to_mask = likes.getcol(pl).indices
      test[cells_to_mask, pl] = 0.

  return train.tocsc(), test.tocsc()

train_set, val_set = train_test_split(likes, 10)

train_set, val_set

!pip install implicit

import implicit

train_set

alpha = 10
conf_train_set = (train_set*alpha).tocsr()
conf_train_set.dtype

#initialize model
!export OPENBLAS_NUM_THREADS=1
mf_model = implicit.als.AlternatingLeastSquares(factors=32, iterations=50)

#make pl_index - filter out playlists that did not get any values in the test set
def make_pl_index(train_set):
  '''
  returns array of indiczes of playlists that have interactions in the test set
  '''
  pl_index = []
  for pl in np.arange(0, train_set.shape[1]): #for each playlist
    if len(train_set.getcol(pl).indices) >= 10: #playlist in train_set must have 10+ songs remaining
      pl_index.append(pl)

  return np.array(pl_index)

def R_precision(recs, mat, pl_index):
  precision = []
  for pl in pl_index:
    top_k_recs = np.array(recs)[:, :500] 
    relevant_songs = mat.getcol(pl).indices
    precision.append(float(len(np.intersect1d(top_k_recs, relevant_songs)))/relevant_songs.shape[0])

  return np.mean(precision)

pl_index = make_pl_index(train_set)
pl_index.shape

train_set.tocsr()

result=[]
for j in [10,20,30]:
  mf_model = implicit.als.AlternatingLeastSquares(factors=j, iterations=50)
  mf_model.fit(conf_train_set)
  recs_train=[]
  for i in range(500):
    recs_train.append(mf_model.recommend(i,train_set.tocsr()[i]))
  mean_precision = R_precision(recs_train, train_set, pl_index)
  result.append({'Nfactor':j,'r-precision':mean_precision})
print(result)

"""LightFM"""

df.artist_uri.value_counts()

n_artist = df.artist_uri.unique().shape[0]
print(f'Num unique artists: {n_artist}')

n_artist_name=df.artist_name.unique().shape[0]
print(f'Num unique artists: {n_artist}')

import numpy as np 
import pandas as pd 
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from scipy import sparse
import random
!pip install lightfm
import lightfm 
from lightfm import LightFM, cross_validation
from lightfm.evaluation import precision_at_k, auc_score
from sklearn.metrics.pairwise import cosine_similarity

df_playlist=df
df_playlist.columns = df_playlist.columns.str.replace('"', '')
# df_playlist.columns = df_playlist.columns.str.replace('name', '')
df_playlist.columns = df_playlist.columns.str.replace(' ', '')
df_playlist.columns

df_playlist = df_playlist.groupby('artist_name').filter(lambda x : len(x)>=50)

df_playlist = df_playlist[df_playlist.groupby('playlist_pid').artist_name.transform('nunique') >= 10]

size = lambda x: len(x)
df_freq = df_playlist.groupby(['playlist_pid', 'artist_name','track_uri']).agg('size').reset_index().rename(columns={0:'freq'})[['playlist_pid', 'artist_name', 'track_uri','freq']].sort_values(['freq'], ascending=False)
df_freq.head()

df_track = pd.DataFrame(df_freq["artist_name"].unique())
df_track = df_track.reset_index()
df_track = df_track.rename(columns={'index':'artist_id', 0:'artist_name'})
df_freq  = pd.merge(df_freq ,df_track, how='inner', on='artist_name')

df_freq

df_track.shape

# def create_interaction_matrix(df,user_col, item_col, rating_col, norm= False, threshold = None):
#     '''
#     Function to create an interaction matrix dataframe from transactional type interactions
#     Required Input -
#         - df = Pandas DataFrame containing user-item interactions
#         - user_col = column name containing user's identifier
#         - item_col = column name containing item's identifier
#         - rating col = column name containing user feedback on interaction with a given item
#         - norm (optional) = True if a normalization of ratings is needed
#         - threshold (required if norm = True) = value above which the rating is favorable
#     Expected output - 
#         - Pandas dataframe with user-item interactions ready to be fed in a recommendation algorithm
#     '''
#     interactions = df.groupby([user_col, item_col])[rating_col] \
#             .sum().unstack().reset_index(). \
#             fillna(0).set_index(user_col)
#     if norm:
#         interactions = interactions.applymap(lambda x: 1 if x > threshold else 0)
#     return interactions

for i in [0.01,0.05,0.1]:
  model = LightFM(learning_rate=i, loss='bpr')
  model.fit(train_set, epochs=10)
  train_precision = precision_at_k(model, train_set, k=10).mean()
  train_auc = auc_score(model, train_set).mean()
  test_auc = auc_score(model, val_set).mean()
  print('learning rate: %.2f'%(i))
  print('Precision: train %.2f' % (train_precision))
  print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))